{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature scaling is a preprocessing technique used in machine learning to standardize the range of independent variables or features of the data. It's important because features in your dataset may have varying scales, and many machine learning algorithms perform better or converge faster when features are on a similar scale. Feature scaling does not change the shape of the original distribution of the data; it simply ensures that the features have similar scales.**\n",
    "\n",
    "**The two common methods for feature scaling are:**\n",
    "\n",
    "**Min-Max scaling (Normalization): This method scales the data to a fixed range—usually between 0 and 1. The formula for Min-Max scaling is:**\n",
    "\n",
    "X normn = X−X min/X max−X min\n",
    "​\n",
    " \n",
    "​\n",
    "\n",
    " \n",
    "\n",
    "where \n",
    "�\n",
    "X is the original value, \n",
    "�\n",
    "min\n",
    "X \n",
    "min\n",
    "​\n",
    "  is the minimum value of the feature, and \n",
    "�\n",
    "max\n",
    "X \n",
    "max\n",
    "​\n",
    "  is the maximum value of the feature.\n",
    "\n",
    "**Standardization (Z-score normalization): This method scales the data so that it has a mean of 0 and a standard deviation of 1. The formula for standardization is:**\n",
    "\n",
    "Xstd =X−μ/σ\n",
    "​\n",
    "\n",
    ",,where \n",
    "�\n",
    "X is the original value, \n",
    "�\n",
    "μ is the mean of the feature, and \n",
    "�\n",
    "σ is the standard deviation of the feature.\n",
    "\n",
    "**Standardization is usually preferred over Min-Max scaling when the data has outliers, as it's less affected by them. However, Min-Max scaling might be more appropriate when there's a strict upper and lower bound on the feature values.**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
